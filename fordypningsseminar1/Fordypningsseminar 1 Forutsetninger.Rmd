---
title: "Fordypningsseminar 1: Forutsetninger for OLS og logistisk regresjon"
output: github_document
---


## Forutsetninger for OLS regresjon

Dere vil se forutsetningene for OLS formulert på litt forskjellige måter i ulike metodetekster. Det er blant annet forskjell på forutsetninger for at OLS skal være forventningsrett og konsistent, og at OLS skal være BLUE. Det er også mulig å formulere de samme forutsetningene i ulik språkdrakt, selv når forutsetningene bygger på de samme matematiske formuleringene. Noen ganger vil dere også se at forutsetningene om restledd er utelatt, andre ganger vil dere kunne se en antagelse om at kurtosen ikke er uendelig stor. Noen vil kategorisere ingen innflytelsesrike observasjoner og ikke perfekt multikolinearitet som antagelser, mens andre vil kategorisere det som problemer/trusler. Dere forholder dere til pensum. Det bør forøvrig nevnes at **Lær deg R** gir en fin gjennomgang av regresjonsdiagnostikk.

**Kritiske aspekter i modellvurdering - OLS:**

1. Ingen utelatt variabelskjevhet
2. Lineær sammenheng mellom variablene
3. Ingen autokorrelasjon/Uavhengige observasjoner
4. Normalfordelte residualer
5. Homoskedastiske residualer
6. Ingen perfekt multikollinearitet
7. Manglende opplysninger(missing values)

Vi skal nå sjekke om forutsetningene for OLS og logistisk regresjonen holder. Vi skal bruke samme datasett som i forrige uke og kjører modellen fra oppgavesett 3a. Først henter vi inn data, gjør de samme omkodingene og kjører modellen.

```{r}

library(tidyverse)

load("aid.RData")

aid <- aid %>% 
  mutate(log_gdp_pr_capita = log(gdp_pr_capita),
         period_fac = as.factor(period),
         region = ifelse(fast_growing_east_asia == 1, "East Asia",
                         ifelse(sub_saharan_africa == 1, "Sub-Saharan Africa", "Other")),
         region = factor(region, levels = c("Other", "Sub-Saharan Africa", "East Asia")))

m5 <- lm(data = aid, 
         gdp_growth ~ log_gdp_pr_capita + ethnic_frac*assasinations + 
           institutional_quality + m2_gdp_lagged + region + policy*aid +
           period_fac, 
         na.action = "na.exclude")

```

## Regresjonsdiagnostikk i R
Jeg anbefaler `car` pakken til John Fox til regresjonsdiagnostikk. Den gir ikke like vakre figurer som `ggplot`, men er veldig lett å bruke for nybegynnere, og inneholder alle slags funksjoner man trenger for regresjonsdiagnostikk. På sikt kan dere lære dere å konstruere disse plottene selv med `ggplot`. Pass imidlertid på at dere forstår hva plotet dere bruker faktisk innebærer (det er lov å spørre om hjelp). I kapittel 6 av boken *An R Companion to Applied Regression* (Fox og Weisberg), gjennomgås diagnostikk med `car` i detalj. En annen pakke som er god, er `lmtest`. Til testing av autokorrelasjon på paneldata er det lettest å bruke pakken `plm`. 

I tillegg til diagnostikken som vi gjør i seminaret, er det fint å se på deskriptiv statistikk, effektplot, samt diskusjon av data. I tillegg til å teste antagelsene over (med unntak av antagelse 1), skal vi også se på innflytelsesrike observasjoner og multikolinearitet.

### Ingen utelatt variabelskjevhet
Hva innebærer denne antagelsen? 

* Dersom vi vil tolke alle variablene i modellen vår substansielt, må alle variabler som påvirker vår avhengige variabel, og som er korrelert med en uavhengig variabel inkluderes i modellen.
* Dersom vi vil tolke en uavhengig variabel, kan vi tenke på de resterende variablene som kontrollvariabler, som er korrelert med uavhengig variabel og påvirker avhengig variabel.

**Merk:** korrelasjon er lineær sammenheng mellom to variabler, ikke et årsaksforhold. Så lenge to variabler påvirker den avhengige variabelen, og de er korrelert (selv om de ikke påvirker hverandre på noe vis), får vi utelatt variabelskjevhet dersom vi ikke kontrollerer for den andre variabelen. 

Denne antagelsen kan vi ikke teste, dersom vi ikke har data for alle variabler. Det betyr at dette først og fremst må begrunnes teoretisk. Det finnes imidlertid metoder for å estimere effekten av utelatte variabler med ulike egenskaper. Denne formen for robusthetstesting kalles *sensivity analysis*, men det er ikke noe vi kommer til å gå inn på her. 

### Lineær sammenheng mellom variablene
Metoden vi bruker for å regne ut lineær regresjon tar blant annet utgangspunkt i kovarians mellom uavhengige variabler og avhengige variabler. I likhet med korrelasjon, er kovarians et mål på lineær sammenheng mellom to variabler. Derfor forutsetter lineær regresjon en lineær sammenheng mellom uavhengig av avhengig variabel. Brudd på denne forutsetningen kan potensielt gi svært missvisende resultater, f.eks. ved å gjøre en U-formet sammenheng om til *ingen lineær sammenheng*.

**Huskregel:** Hver gang vi opphøyer en uavhengig variabel, tillater vi en ekstra *sving* i sammenhengen mellom den avhengige og uavhengige variabelen. 

Dersom hypotesen vår er at det er en positiv sammenheng mellom to variabler, står vi fritt til å legge til andregradsledd og tredjegradsledd, osv, fordi vi ikke påstår at sammenhengen er perfekt lineær, bare at den er positiv. Dette er det vanligste. Vi står dermed fritt til å slenge inn andregrads og tredjegradsledd. Vær imidlertid forsiktig med å opphøye en uavhengig variabel for mye. Da står man i fare for **overfitting**, dvs. å finne en svært spesifikk sammenheng i datasettet ditt, som du ikke finner dersom du samler inn samme type data på nytt. 

I noen tilfeller er hypotesen vår  mer spesifikk, for eksempel at en sammenheng er U-formet (konveks), da må vi teste om: 

1. Vi får en U-formet sammenheng når vi legger inn et annengradsledd.
2. Om regresjonen med et andregradsledd passer til data.

**Viktig:** Dersom dere legger inn andregradsledd eller andre polynomer, husk på å tolke alle leddene for den variabelen sammen. Det er lettest å gjøre dette ved hjelp av plot.


#### Sjekke linearitet i R

Det finnes flere måter å teste linearitetsantagelsen på. Man kan gjøre en grafisk test, ved å plotte residualene til den avhengige variabelen mot residualene til den uavhengige variabelen vi er interessert i. Jeg viser en annen test som gjør samme nytten, men som har noen fordeler.

Vi kan bruke `ggplot()` til å undersøke om sammenhengen mellom en avhengig og en uavhengig variabel er lineær. Ved å lage en spredningsdiagram kan vi undersøke formen på sammenhengen (Introduksjon til statistisk analyse, Christophersen (2013)). Dette kan vi gjøre før vi kjører modellen.

```{r}

library(ggplot2)

ggplot(aid) + 
  geom_point(aes(y = gdp_growth, x = policy)) +
  geom_smooth(aes(y = gdp_growth, x = policy), 
              se = FALSE) +
  theme_bw()

```

Vi kan også bruke funksjonen `ceresPlot()` fra pakken `car` til å teste om sammenhengen mellom en uavhengig og en avhengig variabel er lineær, men da må vi kjøre modellen først. Denne funksjonen fungerer både for lineær regresjon, og for logistisk regresjon (`glm`). Denne funksjonen fungerer imidlertid ikke for regresjon med samspill. 

Det denne funksjonen gjør, er å legge sammen residualene fra en regresjon med parameterestimatet til en variabel (på y-aksen), og plotte mot variabelens verdi. Deretter tegnes det en rosa linje som passer data. 

Dersom sammenhengen ikke er lineær, kan man prøve en transformasjon eller et polynom. 

```{r, eval = FALSE, include=FALSE}
# Kjører modellen uten samspill for å illustrere ceresplot()
model5_usam <- lm(gdp_growth ~ log_gdp_pr_capita + ethnic_frac + assasinations + 
               institutional_quality + m2_gdp_lagged + region + policy + aid +
               period_fac,
             data = aid, na.action = "na.exclude")

stargazer(model5_usam, type = "text")

# installerer og laster inn pakken
# install.packages("car")
library(car)

ceresPlot(model5_usam, "aid")
ceresPlot(model5_usam, "policy")
```

### Uavhengighet/Ingen autokorrelasjon

Denne antagelsen holder dersom vi har et tilfeldig utvalg fra en populasjon, på et tidspunkt. Da vil observasjonene være statistisk uavhengige (alle observasjonene er trukket tilfeldig), og likt distribuert (alle observasjonene er trukket fra samme populasjon). Dersom vi ikke har et slikt utvalg, vil det kunne være sammenhenger mellom observasjoner. Dersom vi f.eks. har data for statsbudsjettet over tid, vil vi trolig se **autokorrelasjon** fra ett år til det neste fordi budsjettet endres inkrementelt. Andre typer avhengighet enn autokorrelasjon er også mulig, som geografisk avhengighet eller tidsperioder.  

#### Sjekke uavhengighet i R
Man kan teste for autkorrelasjon med Durbin-Watson testen. En funksjon for dette er `pdwtest()` fra pakken `plm` - denne fungerer både på tidsserier og paneldata, men krever at du bruker funksjonen `plm()` til å kjøre OLS-modellen din (bruk `?plm` for å se hvordan du kan gjøre dette eller kom på fordypningsseminarene neste uke).`durbinWatsonTest()` fra `car` virker bare på tidsserier, men her kan du bruke `lm()`-objekter.

```{r}
#install.packages("plm")
# library(plm)

# Kjører modellen på ny uten å bevare missingverdier
m5b <- lm(gdp_growth ~ log_gdp_pr_capita + ethnic_frac * assasinations + 
               institutional_quality + m2_gdp_lagged + region + policy * aid +
               period_fac,
             data = aid, na.action = "na.omit")
# Her blir det problemer om vi bevarer na med na.exclude. 

car::durbinWatsonTest(m5b)
```
I utgangspunktet burde vi her kjørt Durbin-Watson-testen fra plm fordi aid-datasettet har paneldata-struktur så dette er bare et eksempel for å vise hvordan dere kan kjøre funksjonen . 

### Normalfordelte residualer:
Residualene fra modellen er normalfordelt og har gjennomsnitt tilnærmet lik 0. 

### Sjekke normalfordelte residualer i R:
Det er flere måter å gjøre dette på. Først kan vi plotte fordelingene til residualene våre og sammenligne med en normalfordeling ved hjelp av `ggplot()`.

```{r}
ggplot() +
  geom_histogram(aes(x = rstandard(m5),
                     y = ..density..)) + 
  stat_function(fun = dnorm, 
                color = "goldenrod2") # Plotter inn en normalfordeling
```


Vi kan teste for normalfordelte residualer ved å plotte studentiserte residualer fra regresjonen vår mot kvantiler fra den kummulative normalfordelingen. Dette kalles qq-plot, og kan kjøres i R med `qqPlot()`.

**Studentiserte residualer:** Alternativ måte å standardisere på, i beregning av varians for hver enkelt observasjon, fjerner man observasjonen. Formålet med dette er at vi får statistisk uavhengihet mellom teller og nevner, noe som lar oss bruke residualene til statistiske tester.

```{r}
car::qqPlot(m5)
```

Vi kan også sjekke skjevhet og kurtose til standardavvikene ved hjelp av funksjonene`kurtosis()` og `skewness()` i pakken moments. 

```{r}
#install.packages("moments")
library(moments)
kurtosis(rstandard(m5), na.rm = TRUE)
skewness(rstandard(m5), na.rm = TRUE)
```
### Homoskedastiske residualer:
Variansen til residualene skal være konstant for ulike nivå av uavhengig variabel. Residualene skal være normalfordelte med jevn spredning over alle predikerte verdier av avhengig variabel. Det vil si at modellen bør predikere like godt når predikerte *y* er høy, som når predikerte *y* er lav (y = fitted values i plottet under).

### Sjekke om vi har homoskedastiske residualer i R:
Vi kan teste for heteroskedastisitet ved hjelp av plot av studentiserte residualer mot standardiserte predikerte verdier fra modellen. Dette kan gjøres med `residualPlot()` i car. Dere kan også lage deres egen versjon med `ggplot()` i stedet. 

```{r}
car::residualPlot(m5)
```


### Ingen perfekt multikolinearitet:
Det skal ikke være en perfekt lineær sammenheng mellom et sett av de uavhengige variablene. Dette fører til at regresjonen ikke lar seg estimere, og skyldes som regel at man har lagt inn dummyvariabler for alle kategorier av en variabel, som en dummy for mann og en for kvinne. Høy multikolinearitet kan også være problematisk, men er ikke en forutsetning for at regresjon ikke skal fungere.

### Sjekke om vi har multikolinearitet i R:
Vi kan teste for multikolinearitet ved hjelp av en vif-test. Funksjonen for dette er `vif()`. Med vif tester vi om det er en sterk lineær sammenheng mellom uavhengige variabler, dersom dette er tilfellet er det gjerne nødvendig med store mengder data for å skille effektene av ulike variabler fra hverandre/få presise estimater (små standardfeil), men bortsett fra å samle mer data er det ikke så mye vi gjøre dersom vi mener begge variablene må være med i modellen. 

```{r}
car::vif(m5)

```
Du kan lese mer om hvilke verdier som er problematiske i kapittel 7 i **Introduksjon til statistisk analyse** eller kapittel 9 i **Lær deg R**. 

### Outliers, leverage og innflytelsesrike observasjoner

Observasjoner med uvanlige/ekstreme verdier på de uvahengige variablene (når man tar høyde for korrelasjonsmønstre), har høy leverage (Vi bruker gjerne hatte-verdier som mål på leverage observasjoner i lineær regresjon). Observasjoner med høy leverage vil ha stor innflytelse på regresjonslinjen, hvis modellen predikerer slike observasjoner dårlig. Observasjoner som blir predikert dårlig av en modell får store residualer. Vi kaller gjerne slike observasjoner "regression outliers" (Studentiserte residualer brukes ofte som mål på "regression outliers"). Innflytelsesrike observasjoner har dermed høy leverage/er dårlig predikert av modellen, og "trekker" regresjonslinjen mot seg med stor kraft. 

Det er ofte lurt å se nærmere på innflytelsesrike enheter og uteliggere, vi kan bruke `influenceIndexPlot()` til å identifisere slike observasjoner. Spesifiser hvor mange observasjoner du vil ha nummerert med argumentet `id = list(n="antall")`. Deretter kan vi se nærmere på disse observasjonene ved hjelp av indeksering. En form for robusthetstesting er å kjøre regresjonen på nytt uten uteliggere og innflytelsesrike observasjoner, for å sjekke om man får samme resultat. Dersom man ikke gjør det, er ikke resultatene dine særlig robuste.

Vi kan også se på Cook's distance, som kombinerer informasjon om uteliggere, leverage og innflytelsesrike observasjoner. `influenceIndexPlot()` gir oss alle disse målene. Disse målene er godt beskrevet i kapittel 9 i **Lær deg R**. 

Dersom du kun er interessert i observasjoners innflytelse på en enkeltvariabel, kan du bruke funksjonen `dfbetas()`, som gir deg hver observasjons innflytelse på koeffisientene til alle variablene i en modell.

```{r}
car::influenceIndexPlot(m5,
                   id = list(n=3))

# Bruker indeksering til å se nærmere på noen av observasjonene
aid[c(39,86), ]

```


### Observasjoner med manglende informasjon (Missing)

Mye kan sies om manglende informasjon (missing) - her viser jeg måter du kan bruke R til å identifisere missing. Jeg viser også noen enkle måter du kan bruke R til å få et inntrykk av konsekvensene av missing på.

I R kan missing være kodet på flere måter. Dersom missing er eksplisitt definert i R, vil vi se missing som `NA` når vi ser på datasettet. Noen ganger leses ikke missing inn som `NA`. Missing på variabler i datasett fra andre statistikkprogramm kan f.eks. leses som `character` med verdi `" "`, eller som `numeric` med verdi `-99`. For å sjekke dette, bør du lese kodebok. Det er ikke sikkert at `" "` bør omkodes til missing. Du kan også se på en tabell, for å identifisere suspekte verdier:


```{r}
table(aid$country) # ingen suspekte verdier
```

Moral: **alltid sjekk kodeboken**, og se på verdiene til data med tabell for å identifisere missing. 

Når du kjører en lineær regresjonsanalyse i R, kastes observasjoner som har manglende informasjon (missing, angitt som `NA` i R) på en eller flere av variablene du legger inn i modellen din ut. Men dersom missing er kodet som f.eks.`999` så vil ikke R automatisk oppdage at dette er en missing verdi. Derfor er det viktig å sjekke hvilke observasjoner som faktisk blir kastet ut av analysen din pga. missing, og hva slags informasjon du faktisk sitter igjen med i analysen din.   

Her er noen nyttige funksjoner for å jobbe missing:

```{r}
aid$reg_complete <- aid %>%
  select(gdp_growth, aid, policy) %>%
  complete.cases()

# Lager variabel som viser hvilke observasjoner som forsvinner i regresjon med de sentrale variablene
# gdp_growth, aid og policy - fin å bruke i plot for å få et inntrykk av hva slags informasjon du mister ved å legge til flere kontrollvariabler.
table(aid$reg_complete) # 47 observasjoner har missing på en eller flere av de tre variablene
```



Vi kan bruke variabelen `reg_complete` til plot. Både spredningsplot og boxplot kan gi god innsikt i hvordan observasjoner med missing skiller seg fra andre. Et annet alternativ, er å se på en logistisk regresjon, med den nye dummyen som avhengig variabel. 

Dersom det er mange observasjoner som kastes ut pga missing, som i eksempelet over, er det lurt å danne seg et inntrykk av konsekvense dette får for analysen din.  Under skisserer jeg noen måter dere kan bruke R på for å lære mer om missingstruktur: 

**Metode 1: korrelasjonsmatriser**

Korrelasjonsmatriser viser korrelasjoner mellom variabler av klassene `numeric` og `integer`.
Dersom vi vil få et raskt inntrykk av konsekvensene av missing i en modell, kan vi lage en korrelasjonsmatrise med variablene som inngår i modellen, og varierer hvordan vi håndterer missing i korrelasjonsmatrisen. Her er et eksempel:

```{r}
# Kjører en modell med litt færre variabler
m1 <- lm(gdp_growth ~ aid*policy + as.factor(period) + ethnic_frac*assasinations, data = aid )
summary(m1) # output viser at 48 observasjoner fjernes pga. missing
```

Lager korrelasjonsmatrise med variablene som inngår:

```{r}
# Siden as.factor(period) lager en dummvariabel for alle perioder unntatt periode 1, må vi gjøre dette for å inkludere denne variabelen i korrelasjonsmatrisen (inkluder gjerne også periode 1 i matrise):

aid$period2 <- ifelse(aid$period==2, 1, 0)
aid$period3 <- ifelse(aid$period==3, 1, 0)
aid$period4 <- ifelse(aid$period==4, 1, 0)
aid$period5 <- ifelse(aid$period==5, 1, 0)
aid$period6 <- ifelse(aid$period==6, 1, 0)
aid$period7 <- ifelse(aid$period==7, 1, 0)
aid$period8 <- ifelse(aid$period==8, 1, 0)

aid %>% 
  select(gdp_growth,aid,policy, ethnic_frac,assasinations,period2,period3,period4,period5,period6,period7) %>%
  cor(, use = "pairwise.complete.obs")
```


```{r}
# Alternativet "pairwise.complete.obs" fjerner bare missing for de enkelte bivariate korrelasjonene
aid %>% 
  select(gdp_growth,aid,policy, ethnic_frac,assasinations,period2,period3,period4,period5,period6,period7) %>%
  cor(, use = "complete.obs")
```


```{r}
# Alternativet "complete.obs" fjerner alle observasjoner som har missing på en av variablene som inngår, mao. det samme som regresjonsanalysen.
```

Ved å sammenligne disse korrelasjonsmatrisene, kan vi få et inntrykk av konsekvensene av å fjerne missing med listwise deletion. 

**Metode 2: Analyse av dummy-variabler for missing**

En alternativ metode å utforske missing i en analyse på, er med funksjonen `complete.cases()`, som gjør en logisk test av om en observasjon har missing. Det var denne vi brukte til å lage variabelen `reg_complete` i stad og kjøre en binomisk logistisk modell. Vi skal snakke mer om logistisk regresjon i morgen så jeg går ikke nærmere inn på dette i dag. 
 
```{r}
miss_mod <- glm(reg_complete ~ aid*policy + as.factor(period), data = aid)
summary(miss_mod) # ingen store forskjeller

# I denne modellen ønsker du ikke signifikante uavhengige variabler
```
Koeffisienten til bistand er negativ og signifikant på 5 % signifikansnivå.Dette indikerer at land som *ikke* fjernes pga missing, får mindre bistand enn land som fjernes.

Vi kunne også definert dummy-variabler for missing på de enkeltvariablene vi er mest interessert i (her: `gdp_growth`, `aid` og `policy`), og gjennomført tilsvarende analyser, ved hjelp av funksjonen `is.na()`. 

I de fleste tilfeller er `ifelse()` en fin funksjon til å definere missing. Statistiske R-funksjoner har stort sett et eller flere argumenter der du kan velge hvordan missing skal håndteres (se for eksempel `?cor`, og argumentene `use` og `na.rm`). Husk på det dere har lært på forelesning, og ta aktive valg om hvordan missing bør håndteres. 

Vi skal ikke bruke modellelementene mer så derfor fjerner vi de fra environment
```{r}
rm(m1, m5, m5b, miss_mod, model5_usam)
```



## Forutsetninger for logistisk regresjon

### Kontroll av forutsetninger
Mange av metodene for diagnostikk for OLS fungerer også for logistisk regresjon. Funksjonene `ceresplot()`, `dfbetas()`, `influenceIndexPlot()`, `vif()` m.m. fungerer også for logistisk regresjon og det samme gjelder missinganalysene. Husk forøvrig på at forutsetninger om homoskedastiske, normalfordelte restledd ikke gjelder logistisk regresjon. I tillegg viser jeg hvordan du kan lage ROC-kurver [her](https://github.com/martigso/stv4020aR/blob/master/Gruppe%201/docs/Introduksjon_seminar4.md). 

Tomme celler vil føre til at modellen ikke lar seg estimere, eller at du ikke får estimert standardfeil/ekstremt høye standardfeil, og er således greit å oppdage. Spredningsplot mellom variabler fra regresjonen kan brukes til å undersøke dette nærmere. En
kan forsøke å løse det ved å forenkle modellen (f.eks. slå sammen kategorier) eller samle inn mer data.


* Vi kan gjøre nøstede likelihood-ratio tester med `anova()`.

```{r}

# Lager dikotom variabel
aid <- aid %>% 
  mutate(gdp_growth_d = ifelse(gdp_growth <= 0, 0, 1))

# Lager et datasett med variablene vi trenger og uten NA
aid.complete <- aid %>% 
  select(gdp_growth_d, aid, policy, period) %>%  # Velger variablene vi skal bruker
  na.omit()                                      # Beholder bare observasjoner uten NA 

# Lager en nullmodel
gm0 <- glm(gdp_growth_d ~ aid, data = aid.complete, 
           family = binomial(link = "logit"))

# Lager en modell med avhengige variabler
gm1 <- glm(gdp_growth_d ~ aid + policy + as.factor(period), data = aid.complete, 
           family = binomial(link = "logit"))
anova(gm0, gm1, test = "LRT") # Sjekk de forskjellige alternativene til test med ?anova.glm

```
*Hva forteller likelihood-ratio testen oss?* 

Likelihood-ratio testen går ut på å signifikansteste forskjellen i -2LL for den fullstendige modellen og en redusert modell. For å kunne gjøre en LR-test så må modellene være nøstede. Positiv og signifikant LR-test betyr at den fullstendige modellen er signifikant bedre tilpasset datamateriale enn den reduserte. 

* Vi kan gjøre hosmer-lemeshow med `hoslem.test()` fra pakken `ResourceSelection`

```{r}
#install.packages("ResourceSelection")
library(ResourceSelection)

# Sjekker nullmodellen
hoslem.test(gm0$y, gm0$fitted.values)

# Sjekker den alternative modellen vår
hoslem.test(gm1$y, gm1$fitted.values)
```

*Hva forteller Hosmer-lemeshow-testen oss?*

I modeller med en uavhengig variabel så tester Hosmer Lemeshow-testen (HL-testen) sammenhengens form. Dersom testen ikke er signifikant har kurven riktig form
(logistisk). Hvis testen er signifikant bør en prøve ulike tilpasninger av modellen som f.eks. annengradsledd, dummy-variabler eller omkoding.

I modeller med flere uavhengige variabler så tester HL-testen modellens generelle tilpasning til datamaterialet ved å sammenlikne observerte
og predikerte verdier for ulike verdikombinasjoner på de uavhengige variablene. Dersom
HL-testen ikke er signifikant har vi en tilfredsstillende tilpasning. Dersom
den er signifikant har vi avvik fra den logistiske kurver eller umodellerte samspill.

* Vi kan regne ut McFadden's pseudo $R^2$ med `pR2` funksjonen fra pakken `pscl`:

```{r}
# install.packages("pscl")
library(pscl)
pR2(gm0)
pR2(gm1)

```

Husk at Pseudo $R^2$ aldri kan tolkes som andel forklart varians. 

**Husk:** I tillegg til formell diagnostikk, må du aldri glemme generelle validitets/metode-vurderinger.

```{r generere_script, eval=FALSE, echo=FALSE}
# knitr::purl("Fordypningsseminar 1 Forutsetninger.Rmd", output = "Fordypningsseminar 1 Forutsetninger.R", documentation = 0)
```


